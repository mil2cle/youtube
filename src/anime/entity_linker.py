"""
Entity Linker - Normalize ‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏•‡∏∞ map ‡∏Å‡∏±‡∏ö series

‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:
- ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (entity extraction)
- Normalize ‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
- Map ‡∏Å‡∏±‡∏ö AniList ID ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
- ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ aliases ‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏†‡∏≤‡∏©‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÜ

‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:
1. ‡πÉ‡∏ä‡πâ AniList Search API ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞ verify ‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞
2. Cache ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API
3. ‡πÉ‡∏ä‡πâ fuzzy matching ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
"""

import re
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any, Tuple
from dataclasses import dataclass, field
from difflib import SequenceMatcher

from rich.console import Console

from src.anime.anilist import AniListClient, AnimeData

console = Console()


@dataclass
class LinkedEntity:
    """‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• entity ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å link ‡πÅ‡∏•‡πâ‡∏ß"""
    
    original_text: str  # ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    normalized_title: str  # ‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà normalize ‡πÅ‡∏•‡πâ‡∏ß
    anilist_id: Optional[int] = None
    mal_id: Optional[int] = None
    confidence: float = 0.0  # 0.0-1.0
    match_type: str = "none"  # exact, fuzzy, partial, none
    anime_data: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô dictionary"""
        return {
            "original_text": self.original_text,
            "normalized_title": self.normalized_title,
            "anilist_id": self.anilist_id,
            "mal_id": self.mal_id,
            "confidence": self.confidence,
            "match_type": self.match_type,
            "anime_data": self.anime_data,
        }


class EntityLinker:
    """
    Entity Linker ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö normalize ‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÅ‡∏•‡∏∞ map ‡∏Å‡∏±‡∏ö series
    
    ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:
        linker = EntityLinker()
        
        # Link ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        result = linker.link_entity("Attack on Titan")
        
        # Link ‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠
        results = linker.link_entities(["AOT", "Demon Slayer", "One Piece"])
        
        # Extract ‡πÅ‡∏•‡∏∞ link ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        entities = linker.extract_and_link("New episode of Attack on Titan announced!")
    """
    
    # Common anime title patterns
    TITLE_PATTERNS = [
        r'"([^"]+)"',  # Quoted titles
        r"'([^']+)'",  # Single quoted titles
        r"„Äå([^„Äç]+)„Äç",  # Japanese quotes
        r"„Äé([^„Äè]+)„Äè",  # Japanese double quotes
    ]
    
    # Common abbreviations and aliases
    KNOWN_ALIASES = {
        "aot": "Attack on Titan",
        "snk": "Shingeki no Kyojin",
        "mha": "My Hero Academia",
        "bnha": "Boku no Hero Academia",
        "kny": "Kimetsu no Yaiba",
        "ds": "Demon Slayer",
        "op": "One Piece",
        "jjk": "Jujutsu Kaisen",
        "csm": "Chainsaw Man",
        "sao": "Sword Art Online",
        "re:zero": "Re:Zero kara Hajimeru Isekai Seikatsu",
        "konosuba": "Kono Subarashii Sekai ni Shukufuku wo!",
        "oregairu": "Yahari Ore no Seishun Love Comedy wa Machigatteiru",
        "danmachi": "Dungeon ni Deai wo Motomeru no wa Machigatteiru Darou ka",
        "fate/stay night": "Fate/stay night",
        "fate/zero": "Fate/Zero",
        "fgo": "Fate/Grand Order",
        "fma": "Fullmetal Alchemist",
        "fmab": "Fullmetal Alchemist: Brotherhood",
        "hxh": "Hunter x Hunter",
        "yyh": "Yu Yu Hakusho",
        "dbz": "Dragon Ball Z",
        "dbs": "Dragon Ball Super",
        "naruto shippuden": "Naruto: Shippuuden",
        "boruto": "Boruto: Naruto Next Generations",
        "bleach tybw": "Bleach: Thousand-Year Blood War",
        "spy x family": "SPY√óFAMILY",
        "spyxfamily": "SPY√óFAMILY",
        "oshi no ko": "Oshi no Ko",
        "frieren": "Sousou no Frieren",
        "solo leveling": "Solo Leveling",
    }
    
    def __init__(
        self,
        cache_dir: Optional[Path] = None,
        cache_ttl_hours: int = 24,
        min_confidence: float = 0.6
    ):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Entity Linker
        
        Args:
            cache_dir: ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö cache (default: data/entity_cache)
            cache_ttl_hours: ‡∏≠‡∏≤‡∏¢‡∏∏‡∏Ç‡∏≠‡∏á cache (‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á)
            min_confidence: ‡∏Ñ‡πà‡∏≤ confidence ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ link
        """
        self.anilist = AniListClient()
        self.min_confidence = min_confidence
        self.cache_ttl = timedelta(hours=cache_ttl_hours)
        
        # Setup cache
        if cache_dir:
            self.cache_dir = cache_dir
        else:
            self.cache_dir = Path(__file__).parent.parent.parent / "data" / "entity_cache"
        
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_file = self.cache_dir / "entity_cache.json"
        
        # Load cache
        self._cache: Dict[str, Dict] = {}
        self._load_cache()
    
    def _load_cache(self):
        """‡πÇ‡∏´‡∏•‡∏î cache ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    
                # Filter expired entries
                now = datetime.now()
                for key, entry in data.items():
                    cached_at = datetime.fromisoformat(entry.get("cached_at", "2000-01-01"))
                    if now - cached_at < self.cache_ttl:
                        self._cache[key] = entry
                
                console.print(f"[dim]üì¶ ‡πÇ‡∏´‡∏•‡∏î entity cache: {len(self._cache)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£[/dim]")
            except Exception as e:
                console.print(f"[yellow]‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î cache: {e}[/yellow]")
    
    def _save_cache(self):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å cache ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå"""
        try:
            with open(self.cache_file, "w", encoding="utf-8") as f:
                json.dump(self._cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            console.print(f"[yellow]‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å cache: {e}[/yellow]")
    
    def _normalize_text(self, text: str) -> str:
        """Normalize ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö"""
        # Lowercase
        text = text.lower()
        
        # Remove special characters except basic punctuation
        text = re.sub(r'[^\w\s\-:!?]', '', text)
        
        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏™‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        norm1 = self._normalize_text(text1)
        norm2 = self._normalize_text(text2)
        
        return SequenceMatcher(None, norm1, norm2).ratio()
    
    def _get_cache_key(self, text: str) -> str:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á cache key ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        return self._normalize_text(text)
    
    def _check_cache(self, text: str) -> Optional[LinkedEntity]:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÉ‡∏ô cache ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà"""
        key = self._get_cache_key(text)
        
        if key in self._cache:
            entry = self._cache[key]
            return LinkedEntity(
                original_text=text,
                normalized_title=entry.get("normalized_title", ""),
                anilist_id=entry.get("anilist_id"),
                mal_id=entry.get("mal_id"),
                confidence=entry.get("confidence", 0.0),
                match_type=entry.get("match_type", "cached"),
                anime_data=entry.get("anime_data"),
            )
        
        return None
    
    def _add_to_cache(self, text: str, entity: LinkedEntity):
        """‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á cache"""
        key = self._get_cache_key(text)
        
        self._cache[key] = {
            "normalized_title": entity.normalized_title,
            "anilist_id": entity.anilist_id,
            "mal_id": entity.mal_id,
            "confidence": entity.confidence,
            "match_type": entity.match_type,
            "anime_data": entity.anime_data,
            "cached_at": datetime.now().isoformat(),
        }
        
        self._save_cache()
    
    def _resolve_alias(self, text: str) -> str:
        """‡πÅ‡∏õ‡∏•‡∏á alias ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°"""
        normalized = self._normalize_text(text)
        
        if normalized in self.KNOWN_ALIASES:
            return self.KNOWN_ALIASES[normalized]
        
        return text
    
    def link_entity(self, text: str, use_cache: bool = True) -> LinkedEntity:
        """
        Link ‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö AniList
        
        Args:
            text: ‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ link
            use_cache: ‡πÉ‡∏ä‡πâ cache ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            
        Returns:
            LinkedEntity ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà link ‡πÅ‡∏•‡πâ‡∏ß
        """
        # Check cache first
        if use_cache:
            cached = self._check_cache(text)
            if cached:
                console.print(f"[dim]üì¶ Cache hit: {text}[/dim]")
                return cached
        
        # Resolve aliases
        resolved_text = self._resolve_alias(text)
        
        # Search on AniList
        results = self.anilist.search_anime(resolved_text, limit=5)
        
        if not results:
            entity = LinkedEntity(
                original_text=text,
                normalized_title=text,
                confidence=0.0,
                match_type="none",
            )
            self._add_to_cache(text, entity)
            return entity
        
        # Find best match
        best_match: Optional[AnimeData] = None
        best_confidence = 0.0
        match_type = "none"
        
        for anime in results:
            # Check all title variants
            titles = [
                anime.title_romaji,
                anime.title_english,
                anime.title_native,
            ]
            
            for title in titles:
                if not title:
                    continue
                
                similarity = self._calculate_similarity(resolved_text, title)
                
                if similarity > best_confidence:
                    best_confidence = similarity
                    best_match = anime
                    
                    if similarity >= 0.95:
                        match_type = "exact"
                    elif similarity >= 0.7:
                        match_type = "fuzzy"
                    else:
                        match_type = "partial"
        
        # Create result
        if best_match and best_confidence >= self.min_confidence:
            entity = LinkedEntity(
                original_text=text,
                normalized_title=best_match.get_best_title(),
                anilist_id=best_match.anilist_id,
                mal_id=best_match.mal_id,
                confidence=best_confidence,
                match_type=match_type,
                anime_data=best_match.to_dict(),
            )
        else:
            entity = LinkedEntity(
                original_text=text,
                normalized_title=text,
                confidence=best_confidence,
                match_type="none",
            )
        
        # Cache result
        self._add_to_cache(text, entity)
        
        return entity
    
    def link_entities(
        self,
        texts: List[str],
        use_cache: bool = True
    ) -> List[LinkedEntity]:
        """
        Link ‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞
        
        Args:
            texts: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ link
            use_cache: ‡πÉ‡∏ä‡πâ cache ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            
        Returns:
            ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ LinkedEntity
        """
        results = []
        
        for text in texts:
            result = self.link_entity(text, use_cache=use_cache)
            results.append(result)
        
        return results
    
    def extract_entities(self, text: str) -> List[str]:
        """
        ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        
        Args:
            text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á entities
            
        Returns:
            ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏û‡∏ö
        """
        entities = []
        
        # Extract quoted titles
        for pattern in self.TITLE_PATTERNS:
            matches = re.findall(pattern, text)
            entities.extend(matches)
        
        # Check for known aliases in text
        text_lower = text.lower()
        for alias, full_name in self.KNOWN_ALIASES.items():
            # Use word boundary to avoid partial matches
            if re.search(rf'\b{re.escape(alias)}\b', text_lower):
                entities.append(full_name)
        
        # Remove duplicates while preserving order
        seen = set()
        unique_entities = []
        for entity in entities:
            normalized = self._normalize_text(entity)
            if normalized not in seen:
                seen.add(normalized)
                unique_entities.append(entity)
        
        return unique_entities
    
    def extract_and_link(
        self,
        text: str,
        use_cache: bool = True
    ) -> List[LinkedEntity]:
        """
        ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞ link ‡∏Å‡∏±‡∏ö AniList
        
        Args:
            text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
            use_cache: ‡πÉ‡∏ä‡πâ cache ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            
        Returns:
            ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ LinkedEntity ‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÅ‡∏•‡∏∞ link ‡πÅ‡∏•‡πâ‡∏ß
        """
        entities = self.extract_entities(text)
        
        if not entities:
            return []
        
        return self.link_entities(entities, use_cache=use_cache)
    
    def add_alias(self, alias: str, full_name: str):
        """
        ‡πÄ‡∏û‡∏¥‡πà‡∏° alias ‡πÉ‡∏´‡∏°‡πà
        
        Args:
            alias: ‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡πà‡∏≠‡∏´‡∏£‡∏∑‡∏≠ alias
            full_name: ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏ï‡πá‡∏°
        """
        self.KNOWN_ALIASES[alias.lower()] = full_name
        console.print(f"[green]‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° alias: {alias} -> {full_name}[/green]")
    
    def get_aliases(self) -> Dict[str, str]:
        """‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ aliases ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        return self.KNOWN_ALIASES.copy()
    
    def clear_cache(self):
        """‡∏•‡πâ‡∏≤‡∏á cache ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        self._cache = {}
        if self.cache_file.exists():
            self.cache_file.unlink()
        console.print("[green]‚úÖ ‡∏•‡πâ‡∏≤‡∏á entity cache ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à[/green]")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á cache"""
        return {
            "total_entries": len(self._cache),
            "cache_file": str(self.cache_file),
            "cache_ttl_hours": self.cache_ttl.total_seconds() / 3600,
        }
