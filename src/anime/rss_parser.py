"""
RSS Feed Parser - ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡∏à‡∏≤‡∏Å RSS feeds ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£

‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
- Anime News Network (ANN) - ‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞‡∏´‡∏•‡∏±‡∏Å
- Crunchyroll News - ‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£‡∏à‡∏≤‡∏Å Crunchyroll
- MyAnimeList News - ‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£‡∏à‡∏≤‡∏Å MAL

‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ RSS feeds ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£ ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£ scrape ‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå
"""

import re
import html
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any
from dataclasses import dataclass, field
from email.utils import parsedate_to_datetime

import requests
from xml.etree import ElementTree as ET
from rich.console import Console

console = Console()


# Whitelisted RSS sources ‡∏û‡∏£‡πâ‡∏≠‡∏° reliability score
RSS_SOURCES = {
    "ann": {
        "name": "Anime News Network",
        "url": "https://www.animenewsnetwork.com/all/rss.xml",
        "reliability_score": 0.95,
        "category": "news",
    },
    "ann_interest": {
        "name": "ANN Interest",
        "url": "https://www.animenewsnetwork.com/interest/rss.xml",
        "reliability_score": 0.90,
        "category": "interest",
    },
    "crunchyroll": {
        "name": "Crunchyroll News",
        "url": "https://www.crunchyroll.com/newsrss",
        "reliability_score": 0.90,
        "category": "news",
    },
    "mal_news": {
        "name": "MyAnimeList News",
        "url": "https://myanimelist.net/rss/news.xml",
        "reliability_score": 0.85,
        "category": "news",
    },
}


@dataclass
class RSSItem:
    """‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å RSS"""
    
    title: str
    link: str
    source: str
    source_name: str
    published_at: Optional[datetime] = None
    description: Optional[str] = None
    raw_text: Optional[str] = None
    categories: List[str] = field(default_factory=list)
    author: Optional[str] = None
    guid: Optional[str] = None
    reliability_score: float = 1.0
    
    def to_dict(self) -> Dict[str, Any]:
        """‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô dictionary"""
        return {
            "title": self.title,
            "link": self.link,
            "source": self.source,
            "source_name": self.source_name,
            "published_at": self.published_at.isoformat() if self.published_at else None,
            "description": self.description,
            "raw_text": self.raw_text,
            "categories": self.categories,
            "author": self.author,
            "guid": self.guid,
            "reliability_score": self.reliability_score,
        }


class RSSFeedParser:
    """
    RSS Feed Parser ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞
    
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ RSS feeds ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô whitelist
    
    ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:
        parser = RSSFeedParser()
        news = parser.fetch_all_sources(days=7)
        
        # ‡∏´‡∏£‡∏∑‡∏≠‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞
        ann_news = parser.fetch_source("ann", days=7)
    """
    
    def __init__(self, timeout: int = 30, custom_sources: Optional[Dict] = None):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á RSS parser
        
        Args:
            timeout: timeout ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö HTTP requests (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
            custom_sources: ‡πÅ‡∏´‡∏•‡πà‡∏á RSS ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏ reliability_score)
        """
        self.timeout = timeout
        self.sources = RSS_SOURCES.copy()
        
        if custom_sources:
            for key, source in custom_sources.items():
                if self._validate_source(source):
                    self.sources[key] = source
                    console.print(f"[green]‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á RSS: {source.get('name', key)}[/green]")
                else:
                    console.print(f"[yellow]‚ö†Ô∏è ‡πÅ‡∏´‡∏•‡πà‡∏á RSS ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á: {key}[/yellow]")
    
    def _validate_source(self, source: Dict) -> bool:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á RSS"""
        required_fields = ["url", "reliability_score"]
        return all(field in source for field in required_fields)
    
    def _clean_html(self, text: str) -> str:
        """‡∏•‡∏ö HTML tags ‡πÅ‡∏•‡∏∞ decode entities"""
        if not text:
            return ""
        
        # Decode HTML entities
        text = html.unescape(text)
        
        # Remove HTML tags
        text = re.sub(r'<[^>]+>', '', text)
        
        # Clean up whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """‡πÅ‡∏õ‡∏•‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å RSS format"""
        if not date_str:
            return None
        
        try:
            # RFC 2822 format (standard RSS)
            return parsedate_to_datetime(date_str)
        except Exception:
            pass
        
        # Try ISO format
        try:
            return datetime.fromisoformat(date_str.replace("Z", "+00:00"))
        except Exception:
            pass
        
        return None
    
    def _parse_feed(self, xml_content: str, source_key: str) -> List[RSSItem]:
        """‡πÅ‡∏õ‡∏•‡∏á XML ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ RSSItem"""
        items = []
        source_info = self.sources.get(source_key, {})
        
        try:
            root = ET.fromstring(xml_content)
            
            # Find all items (RSS 2.0)
            for item in root.findall(".//item"):
                title = item.findtext("title", "")
                link = item.findtext("link", "")
                
                if not title or not link:
                    continue
                
                description = item.findtext("description", "")
                pub_date = item.findtext("pubDate", "")
                author = item.findtext("author", "") or item.findtext("{http://purl.org/dc/elements/1.1/}creator", "")
                guid = item.findtext("guid", "")
                
                # Parse categories
                categories = [cat.text for cat in item.findall("category") if cat.text]
                
                rss_item = RSSItem(
                    title=self._clean_html(title),
                    link=link,
                    source=source_key,
                    source_name=source_info.get("name", source_key),
                    published_at=self._parse_date(pub_date),
                    description=self._clean_html(description)[:500] if description else None,
                    raw_text=self._clean_html(description) if description else None,
                    categories=categories,
                    author=author,
                    guid=guid,
                    reliability_score=source_info.get("reliability_score", 0.5),
                )
                
                items.append(rss_item)
            
            # Try Atom format if no items found
            if not items:
                ns = {"atom": "http://www.w3.org/2005/Atom"}
                for entry in root.findall(".//atom:entry", ns):
                    title = entry.findtext("atom:title", "", ns)
                    link_elem = entry.find("atom:link", ns)
                    link = link_elem.get("href", "") if link_elem is not None else ""
                    
                    if not title or not link:
                        continue
                    
                    content = entry.findtext("atom:content", "", ns) or entry.findtext("atom:summary", "", ns)
                    updated = entry.findtext("atom:updated", "", ns) or entry.findtext("atom:published", "", ns)
                    author_elem = entry.find("atom:author/atom:name", ns)
                    author = author_elem.text if author_elem is not None else ""
                    
                    rss_item = RSSItem(
                        title=self._clean_html(title),
                        link=link,
                        source=source_key,
                        source_name=source_info.get("name", source_key),
                        published_at=self._parse_date(updated),
                        description=self._clean_html(content)[:500] if content else None,
                        raw_text=self._clean_html(content) if content else None,
                        categories=[],
                        author=author,
                        guid=link,
                        reliability_score=source_info.get("reliability_score", 0.5),
                    )
                    
                    items.append(rss_item)
        
        except ET.ParseError as e:
            console.print(f"[red]‚ùå XML Parse Error ({source_key}): {e}[/red]")
        except Exception as e:
            console.print(f"[red]‚ùå Parse Error ({source_key}): {e}[/red]")
        
        return items
    
    def fetch_source(
        self,
        source_key: str,
        days: int = 7,
        limit: Optional[int] = None
    ) -> List[RSSItem]:
        """
        ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á RSS ‡πÄ‡∏â‡∏û‡∏≤‡∏∞
        
        Args:
            source_key: key ‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á RSS (‡πÄ‡∏ä‡πà‡∏ô "ann", "crunchyroll")
            days: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            limit: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            
        Returns:
            ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
        """
        if source_key not in self.sources:
            console.print(f"[red]‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á RSS: {source_key}[/red]")
            console.print(f"[yellow]‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö: {list(self.sources.keys())}[/yellow]")
            return []
        
        source = self.sources[source_key]
        console.print(f"[cyan]üì∞ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å {source['name']}...[/cyan]")
        
        try:
            response = requests.get(source["url"], timeout=self.timeout)
            response.raise_for_status()
            
            items = self._parse_feed(response.text, source_key)
            
            # Filter by date
            cutoff_date = datetime.now() - timedelta(days=days)
            filtered_items = []
            
            for item in items:
                if item.published_at:
                    # Make cutoff_date timezone-aware if item.published_at is
                    if item.published_at.tzinfo is not None:
                        from datetime import timezone
                        cutoff_aware = cutoff_date.replace(tzinfo=timezone.utc)
                        if item.published_at >= cutoff_aware:
                            filtered_items.append(item)
                    else:
                        if item.published_at >= cutoff_date:
                            filtered_items.append(item)
                else:
                    # Include items without date
                    filtered_items.append(item)
            
            # Apply limit
            if limit:
                filtered_items = filtered_items[:limit]
            
            console.print(f"[green]‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å {source['name']} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(filtered_items)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£[/green]")
            
            return filtered_items
            
        except requests.exceptions.Timeout:
            console.print(f"[red]‚ùå Timeout: {source['name']}[/red]")
            return []
        except requests.exceptions.RequestException as e:
            console.print(f"[red]‚ùå Request Error ({source['name']}): {e}[/red]")
            return []
        except Exception as e:
            console.print(f"[red]‚ùå Error ({source['name']}): {e}[/red]")
            return []
    
    def fetch_all_sources(
        self,
        days: int = 7,
        limit_per_source: Optional[int] = None,
        sources: Optional[List[str]] = None
    ) -> List[RSSItem]:
        """
        ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á RSS
        
        Args:
            days: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            limit_per_source: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ï‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á
            sources: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
            
        Returns:
            ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á
        """
        all_items = []
        source_keys = sources or list(self.sources.keys())
        
        console.print(f"[cyan]üì∞ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å {len(source_keys)} ‡πÅ‡∏´‡∏•‡πà‡∏á...[/cyan]")
        
        for source_key in source_keys:
            items = self.fetch_source(source_key, days=days, limit=limit_per_source)
            all_items.extend(items)
        
        # Sort by published date (newest first)
        all_items.sort(
            key=lambda x: x.published_at or datetime.min,
            reverse=True
        )
        
        console.print(f"[green]‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(all_items)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£[/green]")
        
        return all_items
    
    def get_available_sources(self) -> Dict[str, Dict]:
        """‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏´‡∏•‡πà‡∏á RSS ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö"""
        return self.sources.copy()
    
    def add_source(
        self,
        key: str,
        name: str,
        url: str,
        reliability_score: float,
        category: str = "news"
    ) -> bool:
        """
        ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á RSS ‡πÉ‡∏´‡∏°‡πà
        
        Args:
            key: key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á
            name: ‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á
            url: URL ‡∏Ç‡∏≠‡∏á RSS feed
            reliability_score: ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠ (0.0-1.0)
            category: ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á
            
        Returns:
            True ‡∏´‡∏≤‡∏Å‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
        """
        if reliability_score < 0 or reliability_score > 1:
            console.print("[red]‚ùå reliability_score ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0.0-1.0[/red]")
            return False
        
        self.sources[key] = {
            "name": name,
            "url": url,
            "reliability_score": reliability_score,
            "category": category,
        }
        
        console.print(f"[green]‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á RSS: {name}[/green]")
        return True
    
    def remove_source(self, key: str) -> bool:
        """
        ‡∏•‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á RSS
        
        Args:
            key: key ‡∏Ç‡∏≠‡∏á‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏ö
            
        Returns:
            True ‡∏´‡∏≤‡∏Å‡∏•‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
        """
        if key in self.sources:
            del self.sources[key]
            console.print(f"[green]‚úÖ ‡∏•‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á RSS: {key}[/green]")
            return True
        
        console.print(f"[yellow]‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á RSS: {key}[/yellow]")
        return False
