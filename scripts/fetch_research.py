#!/usr/bin/env python3
"""
Fetch Research Script - ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Anime Research ‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ï‡πà‡∏≤‡∏á‡πÜ

‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:
- AniList GraphQL API: trending, seasonal, top anime
- Anime News Network RSS: ‡∏Ç‡πà‡∏≤‡∏ß‡∏™‡∏≤‡∏£‡∏≠‡∏ô‡∏¥‡πÄ‡∏°‡∏∞
- RSS feeds ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô whitelist

‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:
    # ‡∏î‡∏∂‡∏á‡∏ó‡∏∏‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á (7 ‡∏ß‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
    python scripts/fetch_research.py --all
    
    # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ AniList
    python scripts/fetch_research.py --anilist
    
    # ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ RSS feeds
    python scripts/fetch_research.py --rss --days 14
    
    # ‡∏î‡∏∂‡∏á‡πÅ‡∏ö‡∏ö incremental (‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)
    python scripts/fetch_research.py --all --incremental
    
    # ‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà
    python scripts/fetch_research.py --all --start 2024-01-01 --end 2024-01-31
"""

import sys
import argparse
from pathlib import Path
from datetime import datetime, date, timedelta
from typing import Optional, List

# ‡πÄ‡∏û‡∏¥‡πà‡∏° project root ‡πÉ‡∏ô path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn

from src.db.connection import init_db, session_scope
from src.db.repository import ResearchItemRepository, RunLogRepository
from src.anime.anilist import AniListClient
from src.anime.rss_parser import RSSFeedParser
from src.anime.entity_linker import EntityLinker
from src.utils.config import load_config
from src.utils.logger import get_logger

console = Console()
logger = get_logger(__name__)


def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Anime Research ‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ï‡πà‡∏≤‡∏á‡πÜ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:
  %(prog)s --all                    ‡∏î‡∏∂‡∏á‡∏ó‡∏∏‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á (7 ‡∏ß‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î)
  %(prog)s --anilist                ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ AniList
  %(prog)s --rss --days 14          ‡∏î‡∏∂‡∏á RSS feeds 14 ‡∏ß‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
  %(prog)s --all --incremental      ‡∏î‡∏∂‡∏á‡πÅ‡∏ö‡∏ö incremental
  %(prog)s --all --start 2024-01-01 ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        """
    )
    
    # Source selection
    source_group = parser.add_argument_group("‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
    source_group.add_argument(
        "--all",
        action="store_true",
        help="‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á"
    )
    source_group.add_argument(
        "--anilist",
        action="store_true",
        help="‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å AniList API (trending, seasonal, top)"
    )
    source_group.add_argument(
        "--rss",
        action="store_true",
        help="‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å RSS feeds (ANN, Crunchyroll, MAL)"
    )
    source_group.add_argument(
        "--rss-source",
        type=str,
        help="‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å RSS source ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (‡πÄ‡∏ä‡πà‡∏ô ann, crunchyroll)"
    )
    
    # Date range
    date_group = parser.add_argument_group("‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤")
    date_group.add_argument(
        "--days",
        type=int,
        default=7,
        help="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ß‡∏±‡∏ô‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á (default: 7)"
    )
    date_group.add_argument(
        "--start",
        type=str,
        help="‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô (YYYY-MM-DD)"
    )
    date_group.add_argument(
        "--end",
        type=str,
        help="‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏¥‡πâ‡∏ô‡∏™‡∏∏‡∏î (YYYY-MM-DD)"
    )
    date_group.add_argument(
        "--incremental",
        action="store_true",
        default=True,
        help="‡∏î‡∏∂‡∏á‡πÅ‡∏ö‡∏ö incremental ‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (default: true)"
    )
    date_group.add_argument(
        "--no-incremental",
        action="store_true",
        help="‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ incremental mode"
    )
    
    # Options
    options_group = parser.add_argument_group("‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å")
    options_group.add_argument(
        "--limit",
        type=int,
        default=50,
        help="‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ï‡πà‡∏≠‡πÅ‡∏´‡∏•‡πà‡∏á (default: 50)"
    )
    options_group.add_argument(
        "--link-entities",
        action="store_true",
        help="‡∏ó‡∏≥ entity linking ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πà‡∏≤‡∏ß RSS"
    )
    options_group.add_argument(
        "--dry-run",
        action="store_true",
        help="‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
    )
    options_group.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"
    )
    
    return parser.parse_args()


def get_last_research_date(session, source: str) -> Optional[date]:
    """‡∏î‡∏∂‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
    from sqlalchemy import select, func
    from src.db.models import ResearchItem
    
    stmt = (
        select(func.max(ResearchItem.published_at))
        .where(ResearchItem.source == source)
    )
    result = session.execute(stmt).scalar()
    
    if result:
        return result.date() if isinstance(result, datetime) else result
    
    return None


def fetch_anilist_data(
    session,
    limit: int = 50,
    dry_run: bool = False,
    verbose: bool = False
) -> int:
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å AniList API"""
    console.print("\n[bold cyan]üìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å AniList API...[/bold cyan]")
    
    client = AniListClient()
    repo = ResearchItemRepository(session)
    items_saved = 0
    
    # Get current season
    year, season = client.get_current_season()
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
    ) as progress:
        # 1. Trending Anime
        task = progress.add_task("‡∏î‡∏∂‡∏á Trending Anime...", total=None)
        trending = client.get_trending_anime(limit=min(limit, 20))
        progress.update(task, completed=True)
        
        for anime in trending:
            if not dry_run:
                # Check if already exists
                existing = repo.get_by_source_url(f"https://anilist.co/anime/{anime.anilist_id}")
                if existing:
                    continue
                
                repo.create(
                    title=anime.get_best_title(),
                    source="anilist_trending",
                    source_url=anime.site_url,
                    summary=anime.description[:500] if anime.description else None,
                    content=anime.description,
                    keywords={"genres": anime.genres, "tags": [t["name"] for t in anime.tags]},
                    entities={"anime_titles": [anime.title_romaji, anime.title_english, anime.title_native]},
                    linked_series=anime.to_dict(),
                    category="anime",
                    item_type="trending",
                    trend_score=anime.trending / 1000 if anime.trending else 0.5,
                    reliability_score=1.0,
                    anilist_id=anime.anilist_id,
                    mal_id=anime.mal_id,
                    is_actionable=True,
                    is_linked=True,
                    published_at=datetime.now(),
                )
                items_saved += 1
        
        console.print(f"  [green]‚úÖ Trending: {len(trending)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£[/green]")
        
        # 2. Seasonal Anime
        task = progress.add_task(f"‡∏î‡∏∂‡∏á Seasonal Anime ({season} {year})...", total=None)
        seasonal = client.get_seasonal_anime(year, season, limit=min(limit, 30))
        progress.update(task, completed=True)
        
        for anime in seasonal:
            if not dry_run:
                existing = repo.get_by_source_url(f"https://anilist.co/anime/{anime.anilist_id}")
                if existing:
                    continue
                
                repo.create(
                    title=anime.get_best_title(),
                    source="anilist_seasonal",
                    source_url=anime.site_url,
                    summary=anime.description[:500] if anime.description else None,
                    content=anime.description,
                    keywords={"genres": anime.genres, "season": season, "year": year},
                    entities={"anime_titles": [anime.title_romaji, anime.title_english, anime.title_native]},
                    linked_series=anime.to_dict(),
                    category="anime",
                    item_type="seasonal",
                    trend_score=anime.popularity / 100000 if anime.popularity else 0.3,
                    reliability_score=1.0,
                    anilist_id=anime.anilist_id,
                    mal_id=anime.mal_id,
                    is_actionable=True,
                    is_linked=True,
                    published_at=datetime.now(),
                )
                items_saved += 1
        
        console.print(f"  [green]‚úÖ Seasonal ({season} {year}): {len(seasonal)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£[/green]")
        
        # 3. Top Anime by Score
        task = progress.add_task("‡∏î‡∏∂‡∏á Top Anime...", total=None)
        top_anime = client.get_top_anime(sort_by="SCORE_DESC", limit=min(limit, 20))
        progress.update(task, completed=True)
        
        for anime in top_anime:
            if not dry_run:
                existing = repo.get_by_source_url(f"https://anilist.co/anime/{anime.anilist_id}")
                if existing:
                    continue
                
                repo.create(
                    title=anime.get_best_title(),
                    source="anilist_top",
                    source_url=anime.site_url,
                    summary=anime.description[:500] if anime.description else None,
                    content=anime.description,
                    keywords={"genres": anime.genres, "score": anime.average_score},
                    entities={"anime_titles": [anime.title_romaji, anime.title_english, anime.title_native]},
                    linked_series=anime.to_dict(),
                    category="anime",
                    item_type="top_rated",
                    trend_score=anime.average_score / 100 if anime.average_score else 0.5,
                    reliability_score=1.0,
                    anilist_id=anime.anilist_id,
                    mal_id=anime.mal_id,
                    is_actionable=True,
                    is_linked=True,
                    published_at=datetime.now(),
                )
                items_saved += 1
        
        console.print(f"  [green]‚úÖ Top Anime: {len(top_anime)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£[/green]")
    
    if not dry_run:
        session.commit()
    
    return items_saved


def fetch_rss_data(
    session,
    days: int = 7,
    sources: Optional[List[str]] = None,
    limit: int = 50,
    link_entities: bool = False,
    dry_run: bool = False,
    verbose: bool = False
) -> int:
    """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å RSS feeds"""
    console.print("\n[bold cyan]üì∞ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å RSS feeds...[/bold cyan]")
    
    parser = RSSFeedParser()
    repo = ResearchItemRepository(session)
    linker = EntityLinker() if link_entities else None
    items_saved = 0
    
    # Get available sources
    available_sources = parser.get_available_sources()
    
    if sources:
        # Filter to requested sources
        source_keys = [s for s in sources if s in available_sources]
    else:
        source_keys = list(available_sources.keys())
    
    if not source_keys:
        console.print("[yellow]‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÅ‡∏´‡∏•‡πà‡∏á RSS ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏[/yellow]")
        return 0
    
    # Fetch from each source
    for source_key in source_keys:
        source_info = available_sources[source_key]
        console.print(f"\n  [cyan]üì° {source_info['name']}...[/cyan]")
        
        items = parser.fetch_source(source_key, days=days, limit=limit)
        
        for item in items:
            if not dry_run:
                # Check if already exists by URL
                existing = repo.get_by_source_url(item.link)
                if existing:
                    continue
                
                # Extract entities if requested
                entities = None
                linked_series = None
                is_linked = False
                
                if linker and item.raw_text:
                    linked = linker.extract_and_link(item.title + " " + (item.raw_text or ""))
                    if linked:
                        entities = {"anime_titles": [e.original_text for e in linked]}
                        linked_series = [e.to_dict() for e in linked if e.anilist_id]
                        is_linked = bool(linked_series)
                
                repo.create(
                    title=item.title,
                    source=f"rss_{source_key}",
                    source_url=item.link,
                    summary=item.description,
                    content=item.raw_text,
                    keywords={"categories": item.categories},
                    entities=entities,
                    linked_series=linked_series,
                    category="news",
                    item_type="news",
                    trend_score=0.5,
                    reliability_score=item.reliability_score,
                    is_actionable=True,
                    is_linked=is_linked,
                    published_at=item.published_at,
                )
                items_saved += 1
        
        console.print(f"    [green]‚úÖ {len(items)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£[/green]")
    
    if not dry_run:
        session.commit()
    
    return items_saved


def main():
    """Main function"""
    args = parse_args()
    
    # Validate arguments
    if not any([args.all, args.anilist, args.rss, args.rss_source]):
        console.print("[yellow]‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (--all, --anilist, --rss, --rss-source)[/yellow]")
        return 1
    
    # Load config and init DB
    config = load_config()
    init_db(config.database.path)
    
    console.print("[bold]üî¨ Anime Research Fetcher[/bold]")
    console.print(f"[dim]‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}[/dim]")
    
    if args.dry_run:
        console.print("[yellow]‚ö†Ô∏è Dry-run mode: ‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•[/yellow]")
    
    total_items = 0
    
    with session_scope() as session:
        run_repo = RunLogRepository(session)
        
        # Create run log
        run_log = run_repo.create_run(
            run_type="fetch_research",
            triggered_by="cli",
        )
        
        try:
            # Determine date range
            days = args.days
            
            if args.start:
                start_date = datetime.strptime(args.start, "%Y-%m-%d").date()
                if args.end:
                    end_date = datetime.strptime(args.end, "%Y-%m-%d").date()
                else:
                    end_date = date.today()
                days = (end_date - start_date).days
            
            # Fetch AniList data
            if args.all or args.anilist:
                anilist_items = fetch_anilist_data(
                    session,
                    limit=args.limit,
                    dry_run=args.dry_run,
                    verbose=args.verbose,
                )
                total_items += anilist_items
            
            # Fetch RSS data
            if args.all or args.rss or args.rss_source:
                sources = [args.rss_source] if args.rss_source else None
                rss_items = fetch_rss_data(
                    session,
                    days=days,
                    sources=sources,
                    limit=args.limit,
                    link_entities=args.link_entities,
                    dry_run=args.dry_run,
                    verbose=args.verbose,
                )
                total_items += rss_items
            
            # Update run log
            if not args.dry_run:
                run_repo.complete_run(
                    run_log.id,
                    status="completed",
                    items_processed=total_items,
                    items_succeeded=total_items,
                    items_failed=0,
                )
            
            console.print(f"\n[bold green]‚úÖ ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {total_items} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£[/bold green]")
            
        except Exception as e:
            logger.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
            console.print(f"[bold red]‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}[/bold red]")
            
            if not args.dry_run:
                run_repo.fail_run(run_log.id, str(e))
            
            return 1
    
    return 0


# Add helper method to repository
def _add_get_by_source_url_method():
    """‡πÄ‡∏û‡∏¥‡πà‡∏° method get_by_source_url ‡πÉ‡∏´‡πâ ResearchItemRepository"""
    from sqlalchemy import select
    from src.db.models import ResearchItem
    
    def get_by_source_url(self, url: str):
        stmt = select(ResearchItem).where(ResearchItem.source_url == url)
        return self.session.scalar(stmt)
    
    ResearchItemRepository.get_by_source_url = get_by_source_url


_add_get_by_source_url_method()


if __name__ == "__main__":
    sys.exit(main())
